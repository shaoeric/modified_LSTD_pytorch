### 迁移学习概念

迁移学习，指利用数据、任务、模型之间的相似性，将在旧领域学习过的模型，应用于新领域的一种学习过程。

迁移学习的核心问题是，找到新问题和原问题之间的相似性，顺利实现知识的迁移。（举一反三、照猫画虎）

### 为什么要迁移学习？

- 大数据与少标注之间的矛盾：数据量很大，但标注信息很少

- 大数据与弱计算之间的矛盾：穷人买不起计算资源

- 普适化模型与个性化需求之间的矛盾：改造通用模型以满足个性化需求

- 特定应用的需求：应用冷启动，新系统没有足够的数据和标签

### 如何解决？

- 大数据与少标注：寻找一些与目标数据相近的有标注数据，利用这些数据来构建模型，增加我们目标数据的标注
- 大数据与弱计算：模型迁移，在大数据集上训练好的模型，迁移到我们的任务中，针对任务进行微调
- 普适化模型与个性化需求：自适应学习，对普适化模型进行调整
- 特定应用的需求：相似领域知识迁移，利用上述手段，从数据和模型方法上进行迁移

<center>传统机器学习与迁移学习的区别</center>
| 比较项目 | 传统机器学习                 | 迁移学习                   |
| -------- | ---------------------------- | -------------------------- |
| 数据分布 | 训练和测试数据服从相同分布   | 训练和测试数据服从不同分布 |
| 数据标注 | 需要足够的数据标注来训练模型 | 不需要足够的数据分布       |
| 模型     | 每个任务分别建模             | 模型可以在不同任务之间迁移 |

### 负迁移

东施效颦、学习自行车和学习开汽车基本不存在相关性，任务基本完不成。如果两个任务相似性找的不合理，两个领域之间不存在相似性，或者基本不相似，就出现了负迁移。

**负迁移指的是，在源域上学习到的知识，对于目标域上的许欸小产生负面作用**

产生原因：1. 源域和目标域根本不相似。2. 源域和目标域是相似的，但是迁移方法不好，没找到可迁移的成分。

### 迁移学习分类

<img src="C:\Users\hp\AppData\Roaming\Typora\typora-user-images\image-20200223113347085.png" alt="image-20200223113347085" style="zoom: 67%;" />

- 基于样本（实例）的迁移学习：通过权重重用，对源域和目标域的样例进行迁移。相似的样本赋予高权重。

- 基于特征的迁移：对特征进行变换。原本不在同一个空间的源域和目标域的特征，或者在原空间上不相似，想办法把它们变换到一个空间里让它们相似。

- 基于模型的迁移：构建参数共享的模型。神经网络finetune

- 基于关系的迁移：用的少，老师上课学生听课类比公司开会的场景。

同构和异构迁移：如果特征语义和维度都相同就是同构，反之就是异构。不同图片的迁移就是同构，图片到文本的迁移就是异构。

**迁移学习的核心，就是找到相似性**， 如何度量和利用这种相似性？**本质**就是找到一个变换使得源域和目标域距离最小（相似度最大）

- 欧氏距离：$d=\sqrt{(x-y)^T(x-y)}$

- 闵可夫斯基距离：$ d=(||x-y||^p)^{1/p}$

- 马氏距离： $d=\sqrt{(x-y)^T\sum^{-1}(x-y)}$      $\sum$为分布的协方差

- 余弦相似度：$cos(x,y)=\frac{x\cdot y}{|x|\cdot |y|}$

- 互信息

- 皮尔斯相关系数

- Jaccard相关系数

- KL散度和JS距离

- 最大均值差异MMD：$MMD^2(X,Y)=|| \sum_{i=1}^{n_1}\phi(x_i)-\sum_{j=1}^{n_2}\phi{y_j} ||^2_H$

#### 深度迁移学习

深度网络前面几层学习到的是通用特征general feature，也是可以迁移的，效果也比较好。后面的网络偏重于学习任务特定的特征specific feature。finetune可以比较好的克服数据间的差异性。

##### 深度网络自适应

finetune可以帮助节省训练时间，提高学习精度，但是无法处理训练数据和测试**数据分布不同**的情况（finetune假设训练数据和测试数据服从相同的数据分布）。自适应层Adaptation Layer完成源域和目标域数据的自适应，使数据分布更加接近，让网络效果更好。

自适应主要完成两部分工作：

1. 哪些层可以自适应，决定了网络的学习程度
2. 采用说明自适应方法（度量准测），决定了网络的泛化能力

损失定义方式：$ l=l_c(D_s, y_s)+\lambda l_A(D_s,D_t)$

- **DAN的网络结构(迁移领域)**
  ![image-20200223131631150](C:\Users\hp\AppData\Roaming\Typora\typora-user-images\image-20200223131631150.png)<br>

  分布的距离使用的MK-MMD，优化目标是$$min_{\theta}\frac{1}{n_a}\sum_{i=1}^{n_a}J(\theta(x_i^a),y_i^a)+\lambda \sum_{l=l_1}^{l_2}d_k^2(D_s^l,D_t^l) $$

  

- **同时迁移领域和任务的joint CNN architecture for domain and task transfer网络**

  <br>

  任务迁移，就是利用类别之间的相似度（条件分布）。比如杯子与瓶子更相似，与键盘不相似。

  <img src="C:\Users\hp\AppData\Roaming\Typora\typora-user-images\image-20200223135540697.png" alt="image-20200223135540697" style="zoom: 67%;" />

  soft label loss：不仅仅要适配两个domain的marginal distribution，还要考虑类别信息。而target中有大量数据没有label，可以利用source中的label信息。做法是，在网络对source训练时，吧source的每一个样本对于每一个类的概率记下来，求和平均。根据source中的类被分布关系对target做相应的约束。

